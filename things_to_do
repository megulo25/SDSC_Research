- Try to get multi gpu scaling to work:
- Sources of low gpu utilization (slow-down of training)
    * all of the image preprocessing done is expensive on the cpu. Thus the cpu can't feed data as fast as the gpu is processing them.
    * synchronization across all the gpus is also to blame here. Bringing in the weights together it expensive.

- Try experimenting with reducing the number of image preprocessing steps so that it isn't too much on the cpu.

Each of the child class has multiple levels of parent classes that always has birds at the top.
For instance, 
    295 -> 179 -> 1 -> 0
    505 -> 35 -> 11 -> 0

Each folder's name is the child class. The bottom of the chain.
We need to create an output vector that will have all the classes.
Each vector will be 1010 elements long with multiple 1s inserted into each one.
Ex:
    class: 295 -> 179 -> 1 -> 0
    vector: [1 1 0 0 ... 0 0 1  0... 0 1 0 ... 0 0] where the 1s are located at (0, 1, 179, 295)

This is called multi task learning (multi headed monster).

Multi headed monster link: https://blog.manash.me/multi-task-learning-in-keras-implementation-of-multi-task-classification-loss-f1d42da5c3f6

Things to do:
- Set up training script for multi task learning. The loss function is covered. We just need to create a vector for each image.
- Finish code that will generate heatmap.

- Heatmap of the image. Highlight important pixels in image.
    * http://heatmapping.org/